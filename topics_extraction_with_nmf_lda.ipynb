{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topics-extraction-with-nmf-lda.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZvpgix1PdhKvHjKXOlUmb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExoEVKuRYHq",
        "colab_type": "text"
      },
      "source": [
        "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Halc9ObnVZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6ZbwRTZncd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
        "#         Lars Buitnick\n",
        "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lp3s4k5pD0i",
        "colab_type": "text"
      },
      "source": [
        "Author: \n",
        "*   Olivier Grisel <olivier.grisel@ensta.org>\n",
        "*   Lars Buitnick\n",
        "*   Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
        "\n",
        "License: BSD 3 clause"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47IpI2A7n0Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hbX7GoaoUsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B40_XKuSofeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_top_words(model, feature_names, n_top_words):\n",
        "  for topic_idx, topic in enumerate(model.components_):\n",
        "    message = \"Topic #%d: \" % topic_idx\n",
        "    message += \" \".join([feature_names[i]\n",
        "                         for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "    print(message)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg0psk-hqCrv",
        "colab_type": "text"
      },
      "source": [
        "Load the 20 newsgroups dataset and vectorize it.<br>\n",
        "We use a few heuristics to filter out useless terms early on:<br> the posts are stripped of headers, footers and quoted replies, and common English words, words occuring in only one document or in at least 95% of the documents are removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxIDDkZPuMsu",
        "colab_type": "text"
      },
      "source": [
        "### SyntaxError: invalid syntax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8AagbY1qCCn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "24799d00-88ae-4a21-8e34-67cb0ef6c5d6"
      },
      "source": [
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "data, _ fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                           remove=('headers', 'footers', 'quotes'),\n",
        "                           return_X_y=True)\n",
        "data_samples = data[:n_samples]\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-ad7ec8dc4924>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    data, _ fetch_20newsgroups(shuffle=True, random_state=1,\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdd8YlAluPuY",
        "colab_type": "text"
      },
      "source": [
        "### End of SyntaxError"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWg80mrhuSs7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "515e4388-0459-4323-eded-f91d8da2f21e"
      },
      "source": [
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True)\n",
        "data_samples = data[:n_samples]\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "done in 11.002s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-nILSfcyoF-",
        "colab_type": "text"
      },
      "source": [
        "## Use tf-idf features for NMF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyRVS6gzvCTk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7c9c7418-5aa0-4c55-f335-e0444287c675"
      },
      "source": [
        "\n",
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "t0 = time()\n",
        "tf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tf-idf features for NMF...\n",
            "done in 0.343s.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jHuNc6XxE6w",
        "colab_type": "text"
      },
      "source": [
        "### NameError: name 'tf_vectorizer' is not defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbV6a2egxETY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8vfuHwvzeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "ec5943a0-a504-4ee1-8417-920b004c0533"
      },
      "source": [
        "# Use tf (raw term count) features for LDA.\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tfidf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tf features for LDA...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e701edebc250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                    stop_words='english')\n\u001b[1;32m      5\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf_vectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMg_WtBqxHZl",
        "colab_type": "text"
      },
      "source": [
        "### End of NameError"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GtuS3V7xJ9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "05d42e2c-f282-4dad-8f9f-7ab3cf792699"
      },
      "source": [
        "# Use tf (raw term count) features for LDA.\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tf features for LDA...\n",
            "done in 0.379s.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2OetQj2yvqA",
        "colab_type": "text"
      },
      "source": [
        "## Fit the NMF model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9lWwwEv5-G1",
        "colab_type": "text"
      },
      "source": [
        "### NameError: name 'tfidf' is not defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaggVu9IyVp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "0ca57fe6-2fd1-442b-f8ad-38c241b3ffcd"
      },
      "source": [
        "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\\\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "           alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_name = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4476b750ceef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m nmf = NMF(n_components=n_components, random_state=1,\n\u001b[0;32m----> 5\u001b[0;31m            alpha=.1, l1_ratio=.5).fit(tfidf)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Xl-o0v6AqT",
        "colab_type": "text"
      },
      "source": [
        "### End of NameError"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evE74-Vx80M2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "08dd252b-fe95-44f7-cad8-e80a1fbf9608"
      },
      "source": [
        "# Use tf-idf features for NMF.\n",
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "t0 = time()\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# Fit the NMF model\n",
        "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\\\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tf-idf features for NMF...\n",
            "done in 0.410s.\n",
            "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
            "done in 0.381s.\n",
            "\n",
            "Topics in NMF model (Frobenius norm):\n",
            "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
            "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
            "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
            "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
            "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
            "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
            "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
            "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
            "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
            "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShQRXsSa_fII",
        "colab_type": "text"
      },
      "source": [
        "## Fit the NMF model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNUx59e8CM44",
        "colab_type": "text"
      },
      "source": [
        "### TypeError: __init__() got an unexpected keyword argument 'random_states'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCAyrBCP_d2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "63dab550-4ef1-4d64-8b8d-9da3a86d73d5"
      },
      "source": [
        "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\\\n",
        "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting LDA models with tf features, \"\\\n",
        "      \"n_samples=%d and n_features%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_states=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tfidf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
            "done in 1.479s.\n",
            "\n",
            "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
            "Topic #0: people don just like think did say time make know really right said things way ve course didn question probably\n",
            "Topic #1: windows help thanks using hi looking info video dos pc does anybody ftp appreciated mail know advance available use card\n",
            "Topic #2: god does jesus true book christian bible christians religion faith believe life church christ says know read exist lord people\n",
            "Topic #3: thanks know bike interested mail like new car edu heard just price list email hear want cars thing sounds reply\n",
            "Topic #4: 10 00 sale time power 12 new 15 year 30 offer condition 14 16 model 11 monitor 100 old 25\n",
            "Topic #5: space government number public data states earth security water research nasa general 1993 phone information science technology provide blood internet\n",
            "Topic #6: edu file com program soon try window problem remember files sun send library article mike wrong think code win manager\n",
            "Topic #7: game team year games play win season points world division won players nhl flyers toronto case cubs teams ll record\n",
            "Topic #8: drive think hard software disk drives apple computer mac need scsi card don problem read floppy post cable going ii\n",
            "Topic #9: use good just key chip got like ll way clipper doesn keys don better speed stuff want sure going need\n",
            "\n",
            "Fitting LDA models with tf features, n_samples=2000 and n_features1000...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c4c59dab6852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                 \u001b[0mlearning_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'online'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mlearning_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                 random_states=0)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'random_states'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj9UIXlnMVdm",
        "colab_type": "text"
      },
      "source": [
        "### TypeError: __init__() got an unexpected keyword argument 'l1_ratios'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piean6UZCbjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "be2add66-a512-4f0e-a289-22e139cfa651"
      },
      "source": [
        "print(\n",
        "    \"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\\\n",
        "    \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "    % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratios=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting LDA models with tf features, \"\\\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9882b9f62d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m nmf = NMF(n_components=n_components, random_state=1,\n\u001b[1;32m      7\u001b[0m           \u001b[0mbeta_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kullback-leibler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           l1_ratios=.5).fit(tfidf)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'l1_ratios'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqh0s7rhCPXQ",
        "colab_type": "text"
      },
      "source": [
        "### End of TypeError"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPC3LDRfMbpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "db0e713b-9a61-45b3-8b2f-60c8fc0e7904"
      },
      "source": [
        "# Fit the NMF model\n",
        "print(\n",
        "    \"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
        "    \"tf_idf features, n_samples=%d and n_features=%d...\"\n",
        "    % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting LDA models with tf features, \"\\\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf_idf features, n_samples=2000 and n_features=1000...\n",
            "done in 1.469s.\n",
            "\n",
            "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
            "Topic #0: people don just like think did say time make know really right said things way ve course didn question probably\n",
            "Topic #1: windows help thanks using hi looking info video dos pc does anybody ftp appreciated mail know advance available use card\n",
            "Topic #2: god does jesus true book christian bible christians religion faith believe life church christ says know read exist lord people\n",
            "Topic #3: thanks know bike interested mail like new car edu heard just price list email hear want cars thing sounds reply\n",
            "Topic #4: 10 00 sale time power 12 new 15 year 30 offer condition 14 16 model 11 monitor 100 old 25\n",
            "Topic #5: space government number public data states earth security water research nasa general 1993 phone information science technology provide blood internet\n",
            "Topic #6: edu file com program soon try window problem remember files sun send library article mike wrong think code win manager\n",
            "Topic #7: game team year games play win season points world division won players nhl flyers toronto case cubs teams ll record\n",
            "Topic #8: drive think hard software disk drives apple computer mac need scsi card don problem read floppy post cable going ii\n",
            "Topic #9: use good just key chip got like ll way clipper doesn keys don better speed stuff want sure going need\n",
            "\n",
            "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
            "done in 3.918s.\n",
            "\n",
            "Topics in LDA model:\n",
            "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
            "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
            "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
            "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
            "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
            "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
            "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
            "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
            "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
            "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyB_2rL1CQNt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fnan6IlnNn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
        "#         Lars Buitinck\n",
        "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
        "# License: BSD 3 clause\n",
        "\n",
        "from time import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20\n",
        "\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "\n",
        "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
        "# to filter out useless terms early on: the posts are stripped of headers,\n",
        "# footers and quoted replies, and common English words, words occurring in\n",
        "# only one document or in at least 95% of the documents are removed.\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True)\n",
        "data_samples = data[:n_samples]\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# Use tf-idf features for NMF.\n",
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "t0 = time()\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "# Use tf (raw term count) features for LDA.\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()\n",
        "\n",
        "# Fit the NMF model\n",
        "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "# Fit the NMF model\n",
        "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
        "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting LDA models with tf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
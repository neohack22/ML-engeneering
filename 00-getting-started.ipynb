{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/ML-engineering/blob/master/00-getting-started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sciys8A_4l3F"
      },
      "source": [
        "## Getting started with the Converse API in Amazon Bedrock\n",
        "\n",
        "> *This notebook should work well with the **`Python 3`** kernel in SageMaker Studio*\n",
        "\n",
        "In this notebook, we'll explore the basics of the Converse API in Amazon Bedrock. The Converse or ConverseStream API is a unified structured text API action that allows you simplifying the invocations to Bedrock LLMs, using a universal syntax and message structured prompts for any of the supported model providers.\n",
        "\n",
        "Let's start by installing or updating boto3. You just need to run this cell the first time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xvyrbIb4l3I"
      },
      "outputs": [],
      "source": [
        "%pip install --force-reinstall -q -r ./utils/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_zyUb6g4l3J",
        "outputId": "f3e9a07d-dea3-41aa-9b84-8bd67b27689a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running boto3 version: 1.35.13\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import sys\n",
        "print('Running boto3 version:', boto3.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njsIetmD4l3K"
      },
      "source": [
        "Let's define the region and models to use. We can also setup our boto3 client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk8lNNah4l3L",
        "outputId": "65a7f236-bf92-4c47-c13c-a9d337ec991f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using region:  us-west-2\n"
          ]
        }
      ],
      "source": [
        "boto3_session = boto3.session.Session()\n",
        "region = boto3_session.region_name\n",
        "\n",
        "print('Using region: ', region)\n",
        "\n",
        "bedrock = boto3.client(\n",
        "    service_name = 'bedrock-runtime',\n",
        "    region_name = region,\n",
        "    )\n",
        "\n",
        "MODEL_IDS = [\n",
        "    \"amazon.titan-tg1-large\",\n",
        "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation de la session boto3 et du client Bedrock Runtime\n",
        "\n",
        "Ce code Python utilise la librairie boto3 pour interagir avec le service Amazon SageMaker Bedrock Runtime.\n",
        "\n",
        "* **Configuration de la session boto3**\n",
        "  - `boto3_session = boto3.session.Session()`: Cette ligne crée une session boto3. La session gère les informations d'identification et la région AWS par défaut  [➊](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/session.html).\n",
        "  - `region = boto3_session.region_name`: Cette ligne récupère la région AWS configurée par défaut dans la session boto3.\n",
        "\n",
        "* **Création du client Bedrock Runtime**\n",
        "  - `bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)`: Cette ligne crée un client pour interagir avec le service Bedrock Runtime. On précise le nom du service (`bedrock-runtime`) et la région à utiliser (`region`).\n",
        "\n",
        "* **Liste des modèles**\n",
        "  - `MODEL_IDS = [...]`: Cette liste contient les identifiants de plusieurs modèles Bedrock que l'on souhaite peut-être utiliser plus tard dans le code."
      ],
      "metadata": {
        "id": "_1X2Od244xfQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CySUUstb4l3L"
      },
      "source": [
        "We're now ready to setup our Converse API action in Bedrock. Note that we use the same syntax for any model, including the messages-formatted prompts, and the inference parameters. Also note that we read the output in the same way independently of the model used.\n",
        "\n",
        "Optionally, we could define additional model specific request fields that are not common across all providers. For more information on this check the Bedrock Converse API documentation.\n",
        "\n",
        "### Converse for one-shot invocations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqotxu_m4l3M"
      },
      "outputs": [],
      "source": [
        "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
        "    response = \"\"\n",
        "    try:\n",
        "        response = client.converse(\n",
        "            modelId=id,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"text\": prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            inferenceConfig={\n",
        "                \"temperature\": temperature,\n",
        "                \"maxTokens\": max_tokens,\n",
        "                \"topP\": top_p\n",
        "            }\n",
        "            #additionalModelRequestFields={\n",
        "            #}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        result = \"Model invocation error\"\n",
        "    try:\n",
        "        result = response['output']['message']['content'][0]['text'] \\\n",
        "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
        "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
        "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        result = \"Output parsing error\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse de la fonction `invoke_bedrock_model`\n",
        "\n",
        "**Fonctionnalité:**\n",
        "\n",
        "La fonction `invoke_bedrock_model` est conçue pour appeler un modèle Amazon SageMaker Bedrock et renvoyer le résultat de l'invocation, ainsi que des métriques sur la latence et l'utilisation des tokens.\n",
        "\n",
        "**Paramètres:**\n",
        "\n",
        "- `client`: Un objet client boto3 pour interagir avec le service Bedrock Runtime.\n",
        "- `id`: L'identifiant du modèle Bedrock à invoquer.\n",
        "- `prompt`: L'invite ou la question à poser au modèle.\n",
        "- `max_tokens`: Le nombre maximum de tokens à générer dans la réponse.\n",
        "- `temperature`: Un paramètre de contrôle de la randomness de la génération.\n",
        "- `top_p`: Un autre paramètre de contrôle de la randomness de la génération.\n",
        "\n",
        "**Fonctionnement:**\n",
        "\n",
        "1. **Invocation du modèle:**\n",
        "   - La fonction appelle la méthode `converse` du client Bedrock Runtime pour invoquer le modèle spécifié avec l'invite donnée.\n",
        "   - Les paramètres `inferenceConfig` contrôlent la génération de la réponse (nombre maximum de tokens, randomness).\n",
        "\n",
        "2. **Gestion des erreurs:**\n",
        "   - Si une erreur se produit lors de l'invocation du modèle, la fonction renvoie `\"Model invocation error\"`.\n",
        "\n",
        "3. **Extraction et formatage du résultat:**\n",
        "   - Si l'invocation réussit, la fonction extrait le texte de la réponse, ainsi que les métriques de latence et d'utilisation des tokens.\n",
        "   - Le résultat est formaté sous la forme d'une chaîne de caractères.\n",
        "\n",
        "4. **Gestion des erreurs de parsing:**\n",
        "   - Si une erreur se produit lors du parsing du résultat, la fonction renvoie `\"Output parsing error\"`.\n",
        "\n",
        "**Code snippet:**\n",
        "\n",
        "```markdown\n",
        "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
        "    # ... (corps de la fonction)\n",
        "```"
      ],
      "metadata": {
        "id": "RhhyJ7eq5DPN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3YDkE4L4l3M"
      },
      "source": [
        "Finally, we can test our invocation.\n",
        "\n",
        "In this example, we run the same prompt across all the text models supported in Bedrock by the time of writing this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEINTG3S4l3M",
        "outputId": "7847b3aa-2ea1-4c7e-aefa-2f666bd10a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: What is the capital of Italy?\n",
            "\n",
            "Model: amazon.titan-tg1-large\n",
            "The capital of Italy is Rome. It is the fourth most populous city in the European Union.\n",
            "--- Latency: 1375ms - Input tokens:10 - Output tokens:24 ---\n",
            "\n",
            "Model: anthropic.claude-3-haiku-20240307-v1:0\n",
            "The capital of Italy is Rome.\n",
            "--- Latency: 224ms - Input tokens:14 - Output tokens:10 ---\n",
            "\n",
            "Model: anthropic.claude-3-sonnet-20240229-v1:0\n",
            "The capital of Italy is Rome.\n",
            "--- Latency: 283ms - Input tokens:14 - Output tokens:10 ---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = (\"What is the capital of Italy?\")\n",
        "print(f'Prompt: {prompt}\\n')\n",
        "\n",
        "for i in MODEL_IDS:\n",
        "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
        "    print(f'Model: {i}\\n{response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Il définit une variable `prompt` contenant une question sur la capitale de l'Italie.\n",
        "\n",
        "2. Il imprime cette question en utilisant une f-string.\n",
        "\n",
        "3. Ensuite, il y a une boucle `for` qui itère sur une liste ou un itérable appelé `MODEL_IDS`. Pour chaque élément `i` dans `MODEL_IDS` :\n",
        "\n",
        "   - Il appelle une fonction `invoke_bedrock_model` avec les paramètres `bedrock`, `i`, et `prompt`.\n",
        "   - Il imprime le résultat de cette fonction, en précisant le modèle utilisé (`i`) et la réponse obtenue.\n",
        "\n",
        "Voici comment on pourrait améliorer ce code en ajoutant des commentaires explicatifs :\n",
        "\n",
        "```python\n",
        "# Définition de la question\n",
        "prompt = \"What is the capital of Italy?\"\n",
        "print(f'Prompt: {prompt}\\n')\n",
        "\n",
        "# Itération sur les différents modèles\n",
        "for i in MODEL_IDS:\n",
        "    # Appel du modèle Bedrock avec la question\n",
        "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
        "    \n",
        "    # Affichage du résultat pour chaque modèle\n",
        "    print(f'Model: {i}\\n{response}')\n",
        "```\n",
        "\n",
        "utilisé pour tester différents modèles d'IA (probablement des modèles de traitement du langage naturel) avec une même question, et comparer leurs réponses."
      ],
      "metadata": {
        "id": "HOXEt3Cj7fGK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-OX1AXW4l3N"
      },
      "source": [
        "### ConverseStream for streaming invocations\n",
        "\n",
        "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCzpZHWa4l3N"
      },
      "outputs": [],
      "source": [
        "MODEL_IDS = [\n",
        "    \"amazon.titan-tg1-large\",\n",
        "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "éfinit une liste appelée `MODEL_IDS` contenant trois chaînes de caractères. Chaque chaîne représente un identifiant de modèle, pour des modèles d'intelligence artificielle ou de traitement du langage naturel.\n",
        "\n",
        "Voici une explication plus détaillée :\n",
        "\n",
        "1. La liste est définie à l'aide de crochets `[]`.\n",
        "2. Chaque élément de la liste est une chaîne de caractères (string) délimitée par des guillemets doubles `\"`.\n",
        "3. Les éléments sont séparés par des virgules.\n",
        "\n",
        "Pour utiliser cette liste dans votre code Python :\n",
        "\n",
        "```python\n",
        "# Accéder au premier élément de la liste\n",
        "premier_modele = MODEL_IDS[0]\n",
        "print(premier_modele)  # Affichera \"amazon.titan-tg1-large\"\n",
        "\n",
        "# Parcourir tous les éléments de la liste\n",
        "for modele in MODEL_IDS:\n",
        "    print(modele)\n",
        "\n",
        "# Obtenir la longueur de la liste\n",
        "nombre_de_modeles = len(MODEL_IDS)\n",
        "print(f\"Il y a {nombre_de_modeles} modèles dans la liste.\")\n",
        "```\n",
        "\n",
        "Cette structure de données est utile lorsque vous avez besoin de stocker et d'accéder à une collection ordonnée d'identifiants de modèles dans votre programme Python."
      ],
      "metadata": {
        "id": "GSevEbIb7pbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTCd5ngT4l3O"
      },
      "outputs": [],
      "source": [
        "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
        "    response = \"\"\n",
        "    response = client.converse_stream(\n",
        "        modelId=id,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": prompt\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens,\n",
        "            \"topP\": top_p\n",
        "        }\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            sys.stdout.write(chunk['delta']['text'])\n",
        "            sys.stdout.flush()\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette fonction `invoke_bedrock_model_stream` semble être conçue pour interagir avec un modèle de langage, probablement via l'API Bedrock d'Amazon Web Services. Voici les principales caractéristiques :\n",
        "\n",
        "1. La fonction prend plusieurs paramètres :\n",
        "   - `client` : probablement un objet client pour l'API Bedrock\n",
        "   - `id` : l'identifiant du modèle à utiliser\n",
        "   - `prompt` : le texte d'entrée pour le modèle\n",
        "   - `max_tokens`, `temperature`, et `top_p` : des paramètres de configuration pour l'inférence\n",
        "\n",
        "2. Elle utilise la méthode `converse_stream` du client pour envoyer une requête au modèle.\n",
        "\n",
        "3. La réponse est traitée de manière streaming, ce qui signifie que la fonction reçoit et traite la réponse par morceaux au fur et à mesure qu'elle arrive.\n",
        "\n",
        "4. Chaque morceau de texte reçu est immédiatement écrit sur la sortie standard (`sys.stdout`) et le buffer est vidé (`sys.stdout.flush()`), ce qui permet d'afficher la réponse en temps réel.\n",
        "\n",
        "Voici un exemple de comment on pourrait utiliser cette fonction :\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "\n",
        "# Initialiser le client Bedrock\n",
        "bedrock = boto3.client('bedrock-runtime')\n",
        "\n",
        "# Utiliser la fonction\n",
        "invoke_bedrock_model_stream(\n",
        "    client=bedrock,\n",
        "    id=\"votre_modele_id\",\n",
        "    prompt=\"Votre question ou prompt ici\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0.7\n",
        ")\n",
        "```\n",
        "\n",
        "Cette fonction est utile pour des applications qui nécessitent une réponse en temps réel d'un modèle de langage, comme des chatbots ou des assistants IA interactifs."
      ],
      "metadata": {
        "id": "7FBb5JEN73G7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLXouD944l3O",
        "outputId": "d31e634d-0f76-4899-d4b1-ca99b6b70151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: What is the capital of Italy?\n",
            "\n",
            "\n",
            "\n",
            "Model: amazon.titan-tg1-large\n",
            "The capital of Italy is Rome. It is the fourth most populous city in the European Union.\n",
            "\n",
            "Model: anthropic.claude-3-haiku-20240307-v1:0\n",
            "The capital of Italy is Rome.\n",
            "\n",
            "Model: anthropic.claude-3-sonnet-20240229-v1:0\n",
            "The capital of Italy is Rome."
          ]
        }
      ],
      "source": [
        "prompt = (\"What is the capital of Italy?\")\n",
        "print(f'Prompt: {prompt}\\n')\n",
        "\n",
        "for i in MODEL_IDS:\n",
        "    print(f'\\n\\nModel: {i}')\n",
        "    invoke_bedrock_model_stream(bedrock, i, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Une variable `prompt` est définie avec une chaîne de caractères contenant une question.\n",
        "\n",
        "2. Le prompt est affiché à l'aide de la fonction `print()` avec une f-string.\n",
        "\n",
        "3. Une boucle `for` itère sur une liste ou un itérable appelé `MODEL_IDS`.\n",
        "\n",
        "4. Pour chaque modèle dans `MODEL_IDS`, le code affiche le nom du modèle et appelle une fonction `invoke_bedrock_model_stream()`.\n",
        "\n",
        "Pour améliorer ce code et le rendre plus robuste, voici quelques suggestions :\n",
        "\n",
        "```python\n",
        "MODEL_IDS = [\"model1\", \"model2\", \"model3\"]  # Définir la liste des modèles\n",
        "\n",
        "prompt = \"Quelle est la capitale de l'Italie ?\"\n",
        "print(f'Prompt : {prompt}\\n')\n",
        "\n",
        "for model_id in MODEL_IDS:\n",
        "    print(f'\\nModèle : {model_id}')\n",
        "    try:\n",
        "        response = invoke_bedrock_model_stream(bedrock, model_id, prompt)\n",
        "        print(f'Réponse : {response}')\n",
        "    except Exception as e:\n",
        "        print(f'Erreur lors de l\\'invocation du modèle : {e}')\n",
        "```\n",
        "\n",
        "Ce code amélioré :\n",
        "1. Définit explicitement `MODEL_IDS`.\n",
        "2. Utilise un nom de variable plus descriptif dans la boucle.\n",
        "3. Ajoute une gestion des erreurs pour traiter les problèmes potentiels lors de l'appel à `invoke_bedrock_model_stream()`.\n",
        "4. Suppose que `invoke_bedrock_model_stream()` renvoie une réponse et l'affiche.\n",
        "\n",
        "N'oubliez pas d'importer les modules nécessaires et de configurer correctement l'objet `bedrock` avant d'exécuter ce code."
      ],
      "metadata": {
        "id": "zJISMfjD8CXb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIQgrlLt4l3P"
      },
      "source": [
        "As you can see, the Converse API allow us to easily run the invocations with the same syntax across all the models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}